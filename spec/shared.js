const fs = require('fs');
const path = require('path');
const vsctm = require('vscode-textmate');
const oniguruma = require('vscode-oniguruma');
const { assert } = require('chai');

function readFile(p) {
  return new Promise((resolve, reject) => {
    fs.readFile(p, (error, data) => error ? reject(error) : resolve(data));
  })
}

const wasmBin = fs.readFileSync(path.join(__dirname, '../node_modules/vscode-oniguruma/release/onig.wasm')).buffer;
const vscodeOnigurumaLib = oniguruma.loadWASM(wasmBin).then(() => {
  return {
    createOnigScanner(patterns) { return new oniguruma.OnigScanner(patterns); },
    createOnigString(s) { return new oniguruma.OnigString(s); }
  };
});
// Create a registry that can create a grammar from a scope name.
const registry = new vsctm.Registry({
  onigLib: vscodeOnigurumaLib,
  loadGrammar: (scopeName) => {
    if (scopeName === 'source.abl') {
      return readFile('./abl.tmLanguage.json').then(data => vsctm.parseRawGrammar(data.toString(), './abl.tmLanguage.json'))
    }
    console.log(`Unknown scope name: ${scopeName}`);
    return null;
  }
});

afterEach(function () {
  console.log(this.currentTest.file);
})

module.exports = {
  itShouldMatchExpectedScopes: function (statement, expectedTokens) {
    describe(`${statement}`, () => {
      it('should match expected scopes', () => {
        return registry.loadGrammar('source.abl').then(grammar => {
          let tokenResult;

          let lines = statement.split(/\n/g);
          let nbLines = lines.length;
          if (nbLines === 1) {
            //singleline                        
            tokenResult = grammar.tokenizeLine(statement);

            // More human-readable output
            //console.log(`\nTokenizing line: ${statement}`);
            for (let j = 0; j < tokenResult.tokens.length; j++) {
              const token = tokenResult.tokens[j];

              // More human-readable output
              // console.log(` - token from ${token.startIndex} to ${token.endIndex} ` +
              //     `(${statement.substring(token.startIndex, token.endIndex)}) ` +
              //     `with scopes ${token.scopes.join(', ')}`
              // );

              // Formatted as input-values
              var O = token.scopes.map((e) => ('"' + e + '",')).join(' ').replace(/,\s*$/, "");
              console.log(`{ "startIndex": ${token.startIndex}, "endIndex": ${token.endIndex}, "scopes": [${O}] },  // '${statement.substring(token.startIndex, token.endIndex)}'`,);

            }
          } else {
            //multiline, we stack the tokens in an array
            let ruleStack = null;
            tokenResult = {
              tokens: []
            };
            lines.forEach(line => {
              let r = grammar.tokenizeLine(line, ruleStack);
              ruleStack = r.ruleStack;
              tokenResult.tokens.push(r.tokens);

              // More human-readable output
              //console.log(`\nTokenizing line: ${line}`);
              // Formatted as input-values
              console.log(`[`);
              for (let j = 0; j < r.tokens.length; j++) {
                const token = r.tokens[j];
                // More human-readable output
                // console.log(` - token from ${token.startIndex} to ${token.endIndex} ` +
                //     `(${line.substring(token.startIndex, token.endIndex)}) ` +
                //     `with scopes ${token.scopes.join(', ')}`
                // );

                // Formatted as input-values
                var O = token.scopes.map((e) => ('"' + e + '",')).join(' ').replace(/,\s*$/, "");
                console.log(`{ "startIndex": ${token.startIndex}, "endIndex": ${token.endIndex}, "scopes": [${O}] },  // '${line.substring(token.startIndex, token.endIndex)}'`,);

              }
              // Formatted as input-values
              console.log(`],`);

            });
          }
          assert.deepEqual(tokenResult.tokens, expectedTokens, JSON.stringify(tokenResult.tokens));
        })
      })
    })
  }
}